{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#English to Sinhala Transalation with Transforms\n",
        "\n"
      ],
      "metadata": {
        "id": "oCc8LKU_FQ-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "XlyTXePVFdC0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url ='https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "utiv28oPFf1R",
        "outputId": "22f574e8-433d-4a20-c720-2ae0f0789670"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Necessary Library Imports"
      ],
      "metadata": {
        "id": "cTrLD4wBFjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the Data"
      ],
      "metadata": {
        "id": "C8WylncCFlj3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A4a8hSIN2-mG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount the Google Drive"
      ],
      "metadata": {
        "id": "KDw0k_x4FtMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJp4c8if3HaQ",
        "outputId": "475d57cf-2f25-4539-c4e4-8f7518384c1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the data file"
      ],
      "metadata": {
        "id": "auDV_CGBGQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/Deep Learning/Mini Project 03/sin.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKxG68Yj3UAZ",
        "outputId": "7a15a04f-9c72-429c-bd2b-d9732021c985"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\t\tයන්න.\n",
            "Go.\t\tයන්න.\n",
            "Go.\t\tයන්න.\n",
            "Go.\t\tයන්න.\n",
            "Hi.\t\tආයුබෝවන්.\n",
            "Run!\t\tදුවන්න!\n",
            "Run.\t\tදුවන්න.\n",
            "Who?\t\tකවුද?\n",
            "Wow!\t\tවාව්!\n",
            "Fire!\t\tගිනි!\n",
            "Fire!\t\tගිනි!\n",
            "Fire!\t\tගිනි!\n",
            "Help!\t\tඋදව්!\n",
            "Help!\t\tඋදව්!\n",
            "Help!\t\tඋදව්!\n",
            "Jump!\t\tපනින්න!\n",
            "Jump.\t\tපනින්න.\n",
            "Stop!\t\tනවත්වන්න!\n",
            "Stop!\t\tනවත්වන්න!\n",
            "Stop!\t\tනවත්වන්න!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKMzkxhB3clq",
        "outputId": "256ec3fb-6dcc-48e4-a552-236754943494"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom promised.\tටොම් පොරොන්දු විය.\n",
            "Tom promised.\tටොම් පොරොන්දු විය.\n",
            "Tom ran away.\tටොම් පලා ගියේය.\n",
            "Tom relented.\tටොම් පසුබට විය.\n",
            "Tom relented.\tටොම් පසුබට විය.\n",
            "Tom relented.\tටොම් අනුකම්පා කළා.\n",
            "Tom resigned.\tතෝමස් ඉල්ලා අස්විය.\n",
            "Tom saw Mary.\tටොම් මේරිව දැක්කා.\n",
            "Tom screamed.\tටොම් කෑගැසුවා.\n",
            "Tom shot her.\tටොමස් ඔහුට වෙඩි තැබුවේය.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the English and Sinhala translation pairs"
      ],
      "metadata": {
        "id": "uIW4St6OGU7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "\n",
        "    if len(parts) == 2:\n",
        "        english, sinhala = parts\n",
        "        sinhala = \"[start]\" + sinhala + \"[end]\"\n",
        "        text_pairs.append((english, sinhala))\n",
        "    else:\n",
        "\n",
        "        print(\"Skipping line:\", line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H49viQzg3h_e",
        "outputId": "3250749c-946b-4e0a-a7d3-f8263f1cc20a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping line: Go.\t\tයන්න.\n",
            "Skipping line: Go.\t\tයන්න.\n",
            "Skipping line: Go.\t\tයන්න.\n",
            "Skipping line: Go.\t\tයන්න.\n",
            "Skipping line: Hi.\t\tආයුබෝවන්.\n",
            "Skipping line: Run!\t\tදුවන්න!\n",
            "Skipping line: Run.\t\tදුවන්න.\n",
            "Skipping line: Who?\t\tකවුද?\n",
            "Skipping line: Wow!\t\tවාව්!\n",
            "Skipping line: Fire!\t\tගිනි!\n",
            "Skipping line: Fire!\t\tගිනි!\n",
            "Skipping line: Fire!\t\tගිනි!\n",
            "Skipping line: Help!\t\tඋදව්!\n",
            "Skipping line: Help!\t\tඋදව්!\n",
            "Skipping line: Help!\t\tඋදව්!\n",
            "Skipping line: Jump!\t\tපනින්න!\n",
            "Skipping line: Jump.\t\tපනින්න.\n",
            "Skipping line: Stop!\t\tනවත්වන්න!\n",
            "Skipping line: Stop!\t\tනවත්වන්න!\n",
            "Skipping line: Stop!\t\tනවත්වන්න!\n",
            "Skipping line: Wait!\t\tඉන්න!\n",
            "Skipping line: Wait.\t\tඉන්න.\n",
            "Skipping line: Go on.\t\tදිගටම යන්න.\n",
            "Skipping line: Go on.\t\tදිගටම කරගෙන යන්න.\n",
            "Skipping line: Hello!\t\tආයුබෝවන්.\n",
            "Skipping line: I ran.\t\tමම දිව්වා.\n",
            "Skipping line: I ran.\t\tමම දුවමින් සිටියෙමි.\n",
            "Skipping line: I try.\t\tමම උත්සාහ කරනවා.\n",
            "Skipping line: I won!\t\tමම දිනුවා!\n",
            "Skipping line: Oh no!\t\tඅපොයි නෑ!\n",
            "Skipping line: Relax.\t\tඑය සෝඩා සමඟ ගන්න.\n",
            "Skipping line: Smile.\t\tසිනාසෙන්න.\n",
            "Skipping line: Attack!\t\tපහර දෙන්න!\n",
            "Skipping line: Attack!\t\tපහර දෙන්න!\n",
            "Skipping line: Get up.\t\tඔසවන්න.\n",
            "Skipping line: Go now.\t\tදැන්ම යන්න.\n",
            "Skipping line: Got it!\t\tමට එය තිබේ!\n",
            "Skipping line: Got it?\t\tඔබට එය ලැබෙනවාද?\n",
            "Skipping line: Got it?\t\tඔයාට තේරුනාද?\n",
            "Skipping line: He ran.\t\tඔහු දිව්වා.\n",
            "Skipping line: Hop in.\t\tඇතුලට එන්න.\n",
            "Skipping line: Hug me.\t\tමාව සිඹින්න.\n",
            "Skipping line: I fell.\t\tමම දරුණුයි.\n",
            "Skipping line: I know.\t\tමම එය දන්නවා.\n",
            "Skipping line: I left.\t\tමම පිටත් වුණා.\n",
            "Skipping line: I lied.\t\tමම බොරු කිව්වා.\n",
            "Skipping line: I lost.\t\tමට මග හැරුණා.\n",
            "Skipping line: I quit.\t\tමම ඉල්ලා අස්වෙනවා.\n",
            "Skipping line: I quit.\t\tමම අත්හැරියා.\n",
            "Skipping line: I sang.\t\tගායනා කරන්න.\n",
            "Skipping line: I work.\t\tමම ඉන්නේ වැඩ කරමින්.\n",
            "Skipping line: I'm 19.\t\tමට වයස දහනවයයි.\n",
            "Skipping line: I'm up.\t\tමම නැගිට්ටා.\n",
            "Skipping line: Listen.\t\tසවන් දෙන්න.\n",
            "Skipping line: Listen.\t\tසවන් දෙන්න.\n",
            "Skipping line: Listen.\t\tසවන් දෙන්න.\n",
            "Skipping line: No way!\t\tඑය නිකම්ම විය නොහැක!\n",
            "Skipping line: No way!\t\tකොහෙත්ම නැහැ.\n",
            "Skipping line: No way!\t\tකොහෙත්ම නැහැ!\n",
            "Skipping line: No way!\t\tනොහැකියි!\n",
            "Skipping line: No way!\t\tකොහෙත්ම නැහැ!\n",
            "Skipping line: No way!\t\tඒ කිසිවක් නැත!\n",
            "Skipping line: No way!\t\tජරාවක්වත් නෑ!\n",
            "Skipping line: No way!\t\tඅඹ!\n",
            "Skipping line: No way!\t\tමිංගා!\n",
            "Skipping line: No way!\t\tකොහෙත්ම නැහැ!\n",
            "Skipping line: Really?\t\tහා ඇත්තම ද?\n",
            "Skipping line: Really?\t\tඇත්ත?\n",
            "Skipping line: Thanks.\t\tඔයාට ස්තූතියි!\n",
            "Skipping line: Thanks.\t\tඔයාට ස්තූතියි.\n",
            "Skipping line: Try it.\t\tඑය පරීක්ෂා කරන්න.\n",
            "Skipping line: We try.\t\tඅපි එය උත්සාහ කරමු.\n",
            "Skipping line: We won.\t\tඅපි දිනුවා.\n",
            "Skipping line: Why me?\t\tමම නිසාද?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvl07dPkxsTI",
        "outputId": "1f446dc1-ac42-46e4-87bd-6c2452b7b8cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I'm Tom.\", '[start]මම ටොම්.[end]')\n",
            "(\"It's our job.\", '[start]ඒක අපේ වැඩක්.[end]')\n",
            "('Please hurry!', '[start]කරුණාකර ඉක්මන් කරන්න![end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Randomize the data"
      ],
      "metadata": {
        "id": "zfn5AMhnGb1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "0pRoql5G3vlw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the data into training, validation and testing"
      ],
      "metadata": {
        "id": "ai0PPLWAGhcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClpY46Id338E",
        "outputId": "a3409584-cead-41d6-9833-1440872c4ee4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 3426\n",
            "Training set size: 2400\n",
            "Validation set size: 513\n",
            "Testing set size: 513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTGmlxkb37p2",
        "outputId": "fed125f1-aa21-433d-d55f-1553cf57a5a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3426"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Punctuation"
      ],
      "metadata": {
        "id": "6_lk2qTeGk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tYoL52oM3_EU",
        "outputId": "d355c35c-64eb-45a0-9470-deeb583fe10d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{3+5}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4NWmsNo34BT8",
        "outputId": "7ac0f6b2-9ed1-407f-950d-d94d4b8dd4cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vectorizing the English and Sinhala text pairs"
      ],
      "metadata": {
        "id": "Mc-0pbPOGnsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length + 1,\n",
        "  standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "lTA6rGgh4FzC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing datasets for the translation task"
      ],
      "metadata": {
        "id": "nvQ39t8BGrCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "    \"english\": eng,\n",
        "    \"sinhala\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT5tq28h4aeh",
        "outputId": "56ea45a7-344f-4850-ecf3-7ddfe0b1f8ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (32, 20)\n",
            "inputs['sinhala'].shape: (32, 20)\n",
            "targets.shape: (32, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_ds.as_numpy_iterator())[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaQgxLc-4qBT",
        "outputId": "039ac442-37af-4aeb-89c0-fde3d820c5d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'english': array([[  2,  47,  17, ...,   0,   0,   0],\n",
            "       [158,   5,  33, ...,   0,   0,   0],\n",
            "       [ 42,   8,  55, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [ 15,  19,  62, ...,   0,   0,   0],\n",
            "       [870,  12,   0, ...,   0,   0,   0],\n",
            "       [  4,   9, 535, ...,   0,   0,   0]]), 'sinhala': array([[   2,   27, 1244, ...,    0,    0,    0],\n",
            "       [   9,   15,    0, ...,    0,    0,    0],\n",
            "       [ 729,  113,   15, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [1815,    0,    0, ...,    0,    0,    0],\n",
            "       [  17,   11,    0, ...,    0,    0,    0],\n",
            "       [   2, 1392,    0, ...,    0,    0,    0]])}, array([[  27, 1244,    0, ...,    0,    0,    0],\n",
            "       [  15,    0,    0, ...,    0,    0,    0],\n",
            "       [ 113,   15,    0, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   0,    0,    0, ...,    0,    0,    0],\n",
            "       [  11,    0,    0, ...,    0,    0,    0],\n",
            "       [1392,    0,    0, ...,    0,    0,    0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer encoder implemented as a subclassed layer"
      ],
      "metadata": {
        "id": "5oLm_Zf2GwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "        mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "        inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "nkifO00W4uAm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The transformer decorder"
      ],
      "metadata": {
        "id": "dl1A4PayGy-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "      [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "      layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1),\n",
        "        tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    else:\n",
        "      padding_mask = mask\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs,\n",
        "        value=inputs,\n",
        "        key=inputs,\n",
        "        attention_mask=causal_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(\n",
        "        query=attention_output_1,\n",
        "        value=encoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        attention_mask=padding_mask,\n",
        "    )\n",
        "    attention_output_2 = self.layernorm_2(\n",
        "        attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "1RHnHemd5Pa2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Positional Encoding"
      ],
      "metadata": {
        "id": "oakTexdqG33O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "        input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "      input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "4fEReYrw6ih1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "K1h8Xi9n67EY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUXNjDWA69_4",
        "outputId": "7df8f021-29a0-4bc2-af57-dfe7eb4ca261"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "XpPfsZjTG8o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjxDOY4z7Cjw",
        "outputId": "fbb6506e-3b25-45ed-a932-189938a886de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "38/38 [==============================] - 125s 3s/step - loss: 6.3036 - accuracy: 0.3626 - val_loss: 5.1891 - val_accuracy: 0.3753\n",
            "Epoch 2/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 5.0215 - accuracy: 0.3736 - val_loss: 4.7425 - val_accuracy: 0.3833\n",
            "Epoch 3/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 4.5965 - accuracy: 0.3930 - val_loss: 4.4472 - val_accuracy: 0.4016\n",
            "Epoch 4/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 4.1030 - accuracy: 0.4409 - val_loss: 4.1526 - val_accuracy: 0.4755\n",
            "Epoch 5/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 3.6737 - accuracy: 0.4937 - val_loss: 3.8851 - val_accuracy: 0.4923\n",
            "Epoch 6/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 3.2510 - accuracy: 0.5421 - val_loss: 3.7735 - val_accuracy: 0.4938\n",
            "Epoch 7/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 3.1546 - accuracy: 0.5511 - val_loss: 4.0072 - val_accuracy: 0.4528\n",
            "Epoch 8/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 2.6872 - accuracy: 0.6052 - val_loss: 3.6026 - val_accuracy: 0.4982\n",
            "Epoch 9/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 2.4178 - accuracy: 0.6405 - val_loss: 3.4991 - val_accuracy: 0.5187\n",
            "Epoch 10/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 2.1979 - accuracy: 0.6599 - val_loss: 3.4420 - val_accuracy: 0.5238\n",
            "Epoch 11/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 1.9923 - accuracy: 0.6842 - val_loss: 3.4539 - val_accuracy: 0.5311\n",
            "Epoch 12/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 2.3325 - accuracy: 0.6558 - val_loss: 3.4063 - val_accuracy: 0.5384\n",
            "Epoch 13/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 1.6656 - accuracy: 0.7281 - val_loss: 3.4487 - val_accuracy: 0.5333\n",
            "Epoch 14/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 1.5329 - accuracy: 0.7450 - val_loss: 3.3710 - val_accuracy: 0.5457\n",
            "Epoch 15/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 1.3901 - accuracy: 0.7700 - val_loss: 3.3401 - val_accuracy: 0.5552\n",
            "Epoch 16/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 1.2391 - accuracy: 0.7930 - val_loss: 3.2972 - val_accuracy: 0.5530\n",
            "Epoch 17/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 1.0977 - accuracy: 0.8183 - val_loss: 3.3761 - val_accuracy: 0.5604\n",
            "Epoch 18/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 0.9860 - accuracy: 0.8384 - val_loss: 3.3679 - val_accuracy: 0.5706\n",
            "Epoch 19/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 0.8756 - accuracy: 0.8538 - val_loss: 3.3406 - val_accuracy: 0.5779\n",
            "Epoch 20/30\n",
            "38/38 [==============================] - 118s 3s/step - loss: 0.7735 - accuracy: 0.8699 - val_loss: 3.3314 - val_accuracy: 0.5757\n",
            "Epoch 21/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 0.7007 - accuracy: 0.8848 - val_loss: 3.3127 - val_accuracy: 0.5838\n",
            "Epoch 22/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 0.6075 - accuracy: 0.8996 - val_loss: 3.2859 - val_accuracy: 0.5969\n",
            "Epoch 23/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 0.5337 - accuracy: 0.9103 - val_loss: 3.3988 - val_accuracy: 0.5830\n",
            "Epoch 24/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 0.4813 - accuracy: 0.9213 - val_loss: 3.3572 - val_accuracy: 0.5947\n",
            "Epoch 25/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 0.4206 - accuracy: 0.9316 - val_loss: 3.3368 - val_accuracy: 0.5962\n",
            "Epoch 26/30\n",
            "38/38 [==============================] - 122s 3s/step - loss: 0.3771 - accuracy: 0.9401 - val_loss: 3.4379 - val_accuracy: 0.5947\n",
            "Epoch 27/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 0.3372 - accuracy: 0.9502 - val_loss: 3.4218 - val_accuracy: 0.5962\n",
            "Epoch 28/30\n",
            "38/38 [==============================] - 121s 3s/step - loss: 0.3046 - accuracy: 0.9548 - val_loss: 3.3782 - val_accuracy: 0.6020\n",
            "Epoch 29/30\n",
            "38/38 [==============================] - 119s 3s/step - loss: 0.2818 - accuracy: 0.9581 - val_loss: 3.4578 - val_accuracy: 0.5918\n",
            "Epoch 30/30\n",
            "38/38 [==============================] - 120s 3s/step - loss: 0.2494 - accuracy: 0.9626 - val_loss: 3.4679 - val_accuracy: 0.5918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x783347087220>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence =\"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization(\n",
        "        [decoded_sentence])[:,:-1]\n",
        "    predictions = transformer(\n",
        "        [tokenized_input_sentence,tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0,i,:])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPyzR4UT7N0M",
        "outputId": "3f223cb5-84c3-45c8-a599-c9d9321495fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Eat slowly.\n",
            "[start] කනවා[end]       වනු වන්න[end]  විය[end]         \n",
            "-\n",
            "What a dope!\n",
            "[start] මොන දවසක්ද[end]      පලයන්[end]   වේ[end]      ඇත[end]   \n",
            "-\n",
            "They've left.\n",
            "[start] ගියා[end]      ගියා[end]    ගියා[end]         \n",
            "-\n",
            "Is Tom cured?\n",
            "[start] ඉන්නවද[end]       වේ[end]   වේ[end]         \n",
            "-\n",
            "Do come in.\n",
            "[start] එන්න[end]      එන්න[end] යන්න[end]            \n",
            "-\n",
            "Leave now.\n",
            "[start] දිනුවා[end]     යන්න[end]  යන්න[end]   යන්න[end]         \n",
            "-\n",
            "Let's start.\n",
            "[start] පටන් ගන්න[end]         ඇත[end]         \n",
            "-\n",
            "Keep dancing.\n",
            "[start] දිගටම කරගෙන යන්න[end]     නොවන්න[end]   ගන්න[end]         \n",
            "-\n",
            "Tom's upset.\n",
            "[start] කලින් එනවා[end]      නොවන්න[end]   විය[end]         \n",
            "-\n",
            "Here she is!\n",
            "[start] ගන්න[end]       මෙහි වන්න[end]  විය[end]         \n",
            "-\n",
            "Shadow him.\n",
            "[start] උත්සාහ කරන්න[end]      කරන්න[end]   විය[end]         \n",
            "-\n",
            "I resigned.\n",
            "[start] ඉල්ලා අස්විය[end]     ඇත[end] ඉල්ලා ගන්න[end]  ඇත[end]      ඇත[end]   \n",
            "-\n",
            "Say cheese.\n",
            "[start] කියන්න[end]      කියන්න[end]    කියන්න[end]         \n",
            "-\n",
            "Where am I?\n",
            "[start] ආවා[end]       ආවා[end]   විය[end]         \n",
            "-\n",
            "Come with us.\n",
            "[start] එන්න[end]      එන්න[end] ආවා[end]   යන්න[end]         \n",
            "-\n",
            "Is that OK?\n",
            "[start] එය නිවැරදිව තේරුම් ගත්තාද[end]  විය[end] ඇත[end] නොවන්න[end]   සිටී[end]         \n",
            "-\n",
            "They've left.\n",
            "[start] ගියා[end]      ගියා[end]    ගියා[end]         \n",
            "-\n",
            "Leave now.\n",
            "[start] දිනුවා[end]     යන්න[end]  යන්න[end]   යන්න[end]         \n",
            "-\n",
            "Leave me.\n",
            "[start] යන්න[end]       යන්න[end]   යන්න[end]         \n",
            "-\n",
            "I'm studying.\n",
            "[start] ආවා[end]          යන්න[end]         \n"
          ]
        }
      ]
    }
  ]
}